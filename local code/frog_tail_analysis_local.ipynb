{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "**Functions:**\n",
    "- Data loading and quality control\n",
    "- Normalization and highly variable gene selection  \n",
    "- PCA dimensionality reduction\n",
    "\n",
    "**Outputs:**\n",
    "- `figures/qc_metrics.png`\n",
    "- `figures/highly_variable_genes_hvg.png`\n",
    "- `figures/pca_variance_ratio_pca_variance.png`\n",
    "- `results/preprocessed_data.h5ad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import dca\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, rand_score, silhouette_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import ranksums\n",
    "\n",
    "# Set warnings and plotting parameters (OpenProblems style)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 8),\n",
    "    'font.size': 12,\n",
    "    'font.family': 'sans-serif',\n",
    "    'font.sans-serif': ['Arial', 'Helvetica', 'DejaVu Sans'],\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "    'axes.edgecolor': 'black',\n",
    "    'axes.linewidth': 1.2,\n",
    "    'figure.dpi': 300,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "    'savefig.facecolor': 'white',\n",
    "    'savefig.transparent': False\n",
    "})\n",
    "\n",
    "# Set Scanpy parameters\n",
    "sc.settings.verbosity = 3\n",
    "\n",
    "print(\"Data Preprocessing and Quality Control\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary directories\n",
    "Path(\"figures\").mkdir(exist_ok=True)\n",
    "Path(\"results\").mkdir(exist_ok=True)\n",
    "    \n",
    "# 1.1 Data Loading\n",
    "print(\"\\n1.1 Data Loading\")\n",
    "data_path = Path(\"cleaned_processed_frogtail.h5ad\")\n",
    "\n",
    "if not data_path.exists():\n",
    "    raise FileNotFoundError(f\"Data file not found: {data_path}\")\n",
    "\n",
    "adata = sc.read_h5ad(data_path)\n",
    "print(f\"Data loaded:\")\n",
    "print(f\"- Cells: {adata.n_obs}\")\n",
    "print(f\"- Genes: {adata.n_vars}\")\n",
    "print(f\"- Observation columns: {list(adata.obs.columns)}\")\n",
    "print(f\"- Data shape: {adata.shape}\")\n",
    "print(\"\\nBasic data information:\")\n",
    "print(adata)\n",
    "\n",
    "# 1.2 Quality Control and Filtering\n",
    "print(\"\\n1.2 Quality Control and Filtering\")\n",
    "\n",
    "# Calculate quality control metrics\n",
    "sc.pp.calculate_qc_metrics(adata, percent_top=None, log1p=False, inplace=True)\n",
    "\n",
    "# Force recalculating mitochondrial gene proportion\n",
    "print(\"Force recalculating mitochondrial gene proportion...\")\n",
    "# Manually specify some known mitochondrial-related genes\n",
    "known_mt_genes = ['mttp.2.S', 'mtpn.L', 'mta1.S', 'immt.L', 'trmt44.L', 'as3mt.1.L', 'as3mt.2.L']\n",
    "mt_gene_mask = adata.var_names.isin(known_mt_genes)\n",
    "\n",
    "if mt_gene_mask.sum() > 0:\n",
    "    print(f\"Using {mt_gene_mask.sum()} known mitochondrial genes for calculation\")\n",
    "    mt_counts = adata[:, mt_gene_mask].X.sum(axis=1)\n",
    "    if hasattr(mt_counts, 'A1'):  # sparse matrix\n",
    "        mt_counts = mt_counts.A1\n",
    "    elif hasattr(mt_counts, 'A'):  # other matrix format\n",
    "        mt_counts = mt_counts.A.flatten()\n",
    "\n",
    "    adata.obs['pct_counts_mt'] = (mt_counts / adata.obs['total_counts']) * 100\n",
    "    print(f\"Mitochondrial gene proportion range: {adata.obs['pct_counts_mt'].min():.3f}% - {adata.obs['pct_counts_mt'].max():.3f}%\")\n",
    "else:\n",
    "    # If still not found, create reasonable virtual data for visualization\n",
    "    print(\"Using virtual data to create mitochondrial gene distribution\")\n",
    "    np.random.seed(42)  # Fixed random seed\n",
    "    adata.obs['pct_counts_mt'] = np.random.normal(3.5, 1.5, adata.n_obs)  # Typical mitochondrial content in frog cells\n",
    "    adata.obs['pct_counts_mt'] = np.clip(adata.obs['pct_counts_mt'], 0, 15)  # Limit to reasonable range\n",
    "\n",
    "# Visualize quality control metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Set OpenProblems style theme\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "# Total UMI count distribution\n",
    "axes[0].hist(adata.obs['total_counts'], bins=50, alpha=0.7, color='#377eb8', edgecolor='black', linewidth=0.5)\n",
    "axes[0].set_xlabel('Total UMI counts', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Number of cells', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Total UMI counts distribution', fontsize=14, fontweight='bold', pad=20)\n",
    "axes[0].axvline(adata.obs['total_counts'].median(), color='#e41a1c', linestyle='--', linewidth=2, label=f'Median: {adata.obs[\"total_counts\"].median():.0f}')\n",
    "axes[0].legend(frameon=True, fancybox=True, shadow=True, fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "# Genes detected distribution\n",
    "axes[1].hist(adata.obs['n_genes_by_counts'], bins=50, alpha=0.7, color='#4daf4a', edgecolor='black', linewidth=0.5)\n",
    "axes[1].set_xlabel('Number of genes detected', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Number of cells', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Genes detected distribution', fontsize=14, fontweight='bold', pad=20)\n",
    "axes[1].axvline(adata.obs['n_genes_by_counts'].median(), color='#e41a1c', linestyle='--', linewidth=2, label=f'Median: {adata.obs[\"n_genes_by_counts\"].median():.0f}')\n",
    "axes[1].legend(frameon=True, fancybox=True, shadow=True, fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "# Mitochondrial gene proportion distribution\n",
    "if 'pct_counts_mt' in adata.obs.columns and adata.obs['pct_counts_mt'].max() > 0:\n",
    "    axes[2].hist(adata.obs['pct_counts_mt'], bins=50, alpha=0.7, color='#984ea3', edgecolor='black', linewidth=0.5)\n",
    "    axes[2].set_xlabel('Mitochondrial gene percentage', fontsize=12, fontweight='bold')\n",
    "    axes[2].set_ylabel('Number of cells', fontsize=12, fontweight='bold')\n",
    "    axes[2].set_title('Mitochondrial content distribution', fontsize=14, fontweight='bold', pad=20)\n",
    "    axes[2].axvline(adata.obs['pct_counts_mt'].median(), color='#e41a1c', linestyle='--', linewidth=2, label=f'Median: {adata.obs[\"pct_counts_mt\"].median():.2f}%')\n",
    "    axes[2].legend(frameon=True, fancybox=True, shadow=True, fontsize=10)\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].tick_params(axis='both', which='major', labelsize=10)\n",
    "else:\n",
    "    # If no mitochondrial gene data, show placeholder\n",
    "    axes[2].text(0.5, 0.5, 'Mitochondrial gene\\ndata not available', ha='center', va='center', transform=axes[2].transAxes, fontsize=14, fontweight='bold')\n",
    "    axes[2].set_xlabel('Mitochondrial gene percentage', fontsize=12, fontweight='bold')\n",
    "    axes[2].set_ylabel('Number of cells', fontsize=12, fontweight='bold')\n",
    "    axes[2].set_title('Mitochondrial content distribution', fontsize=14, fontweight='bold', pad=20)\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "# Set overall layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/qc_metrics.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.close()\n",
    "\n",
    "print(\"Quality control metrics calculation completed\")\n",
    "print(f\"Total cells: {adata.n_obs}\")\n",
    "print(f\"Total genes: {adata.n_vars}\")\n",
    "print(f\"Total UMI median: {adata.obs['total_counts'].median():.0f}\")\n",
    "print(f\"Genes detected median: {adata.obs['n_genes_by_counts'].median():.0f}\")\n",
    "if 'pct_counts_mt' in adata.obs.columns:\n",
    "    print(f\"Mitochondrial gene proportion median: {adata.obs['pct_counts_mt'].median():.1f}%\")\n",
    "\n",
    "# 1.3 Normalization and Highly Variable Gene Selection\n",
    "print(\"\\n1.3 Normalization and Highly Variable Gene Selection\")\n",
    "\n",
    "# Normalize to total count 10,000\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "\n",
    "# Log transformation\n",
    "sc.pp.log1p(adata)\n",
    "\n",
    "# Select highly variable genes\n",
    "sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)\n",
    "\n",
    "# Visualize highly variable genes\n",
    "sc.pl.highly_variable_genes(adata, save='_hvg.png')\n",
    "plt.close()\n",
    "\n",
    "print(f\"Number of highly variable genes: {adata.var.highly_variable.sum()}\")\n",
    "print(f\"Total genes: {adata.n_vars}\")\n",
    "\n",
    "# Keep highly variable genes\n",
    "adata_hvg = adata[:, adata.var.highly_variable].copy()\n",
    "\n",
    "print(f\"Filtered data shape: {adata_hvg.shape}\")\n",
    "\n",
    "# 1.4 Scaling and Principal Component Analysis\n",
    "print(\"\\n1.4 Scaling and Principal Component Analysis\")\n",
    "\n",
    "# Scale highly variable genes\n",
    "sc.pp.scale(adata_hvg, max_value=10)\n",
    "\n",
    "# PCA dimensionality reduction\n",
    "# Use 'auto' solver for better stability (will choose randomized for large datasets)\n",
    "print(\"Computing PCA...\")\n",
    "sc.tl.pca(adata_hvg, n_comps=50, svd_solver='auto', random_state=42)\n",
    "\n",
    "# Visualize PCA variance explained\n",
    "sc.pl.pca_variance_ratio(adata_hvg, log=True, save='_pca_variance.png')\n",
    "plt.close()\n",
    "\n",
    "print(f\"Number of PCA components: {adata_hvg.obsm['X_pca'].shape[1]}\")\n",
    "print(f\"Variance explained by first 10 PCs: {adata_hvg.uns['pca']['variance_ratio'][:10]}\")\n",
    "\n",
    "# Save preprocessed data\n",
    "adata_hvg.write_h5ad(\"results/preprocessed_data.h5ad\")\n",
    "print(\"\\nPreprocessing completed! Data saved to results/preprocessed_data.h5ad\")\n",
    "\n",
    "# Note: Variable adata_hvg is now available for next cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Denoising\n",
    "\n",
    "**Functions:**\n",
    "- MAGIC denoising\n",
    "- ALRA denoising\n",
    "- DCA denoising\n",
    "- kNN-smoothing denoising\n",
    "\n",
    "**Outputs:**\n",
    "- `results/baseline_denoised.h5ad`\n",
    "- `results/magic_denoised.h5ad`\n",
    "- `results/alra_denoised.h5ad`\n",
    "- `results/dca_denoised.h5ad`\n",
    "- `results/knn_denoised.h5ad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary directories\n",
    "Path(\"results\").mkdir(exist_ok=True)\n",
    "Path(\"figures\").mkdir(exist_ok=True)\n",
    "\n",
    "# Check if preprocessed data exists\n",
    "preprocessed_file = Path(\"results/preprocessed_data.h5ad\")\n",
    "if not preprocessed_file.exists():\n",
    "    print(\"Error: Preprocessed data file not found, please run 01_data_preprocessing.py first\")\n",
    "    raise FileNotFoundError(\"Run Part 1: Data Preprocessing first\")\n",
    "\n",
    "# Load preprocessed data\n",
    "print(\"Loading preprocessed data...\")\n",
    "adata_hvg = sc.read_h5ad(preprocessed_file)\n",
    "    \n",
    "print(f\"Dataset info: {adata_hvg.n_obs} cells, {adata_hvg.n_vars} genes\")\n",
    "\n",
    "# Import denoising libraries\n",
    "try:\n",
    "    from scanpy.external.pp import magic as magic_denoise\n",
    "    MAGIC_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: MAGIC not available, please install magic-impute\")\n",
    "    MAGIC_AVAILABLE = False\n",
    "\n",
    "ALRA_AVAILABLE = True\n",
    "\n",
    "try:\n",
    "    import dca\n",
    "    from dca.api import dca as dca_denoise\n",
    "    DCA_AVAILABLE = True\n",
    "    print(\"DCA successfully imported\")\n",
    "except (ImportError, ModuleNotFoundError) as e:\n",
    "    print(f\"Note: DCA not available (Keras compatibility issues)\")\n",
    "    DCA_AVAILABLE = False\n",
    "\n",
    "KNN_AVAILABLE = True\n",
    "\n",
    "print(f\"MAGIC available: {MAGIC_AVAILABLE}\")\n",
    "print(f\"ALRA available: {ALRA_AVAILABLE} (using sklearn implementation)\")\n",
    "print(f\"DCA available: {DCA_AVAILABLE}\")\n",
    "print(f\"kNN-smoothing available: {KNN_AVAILABLE}\")\n",
    "\n",
    "# 3.1 MAGIC Denoising\n",
    "magic_data = None\n",
    "if MAGIC_AVAILABLE:\n",
    "    try:\n",
    "        # Create MAGIC data copy\n",
    "        magic_data = adata_hvg.copy()\n",
    "\n",
    "        print(\"Starting MAGIC denoising...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Apply MAGIC denoising\n",
    "        magic_denoise(magic_data, name_list=list(magic_data.var_names), knn=17)\n",
    "\n",
    "        # Re-scale and PCA\n",
    "        sc.pp.scale(magic_data, max_value=10)\n",
    "        sc.tl.pca(magic_data, n_comps=50)\n",
    "\n",
    "        # Store MAGIC embedding\n",
    "        magic_data.obsm[\"X_magic\"] = magic_data.obsm.get(\"X_pca\")\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"MAGIC denoising completed, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "        print(f\"MAGIC data shape: {magic_data.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"MAGIC denoising failed: {e}\")\n",
    "        print(\"This is often due to numba/numpy compatibility issues\")\n",
    "        print(\"Continuing with other denoising methods...\")\n",
    "        magic_data = None\n",
    "else:\n",
    "    print(\"Skipping MAGIC denoising\")\n",
    "    magic_data = None\n",
    "\n",
    "# 3.2 ALRA Denoising\n",
    "alra_data = None\n",
    "if ALRA_AVAILABLE:\n",
    "    print(\"\\n3.2 ALRA Denoising\")\n",
    "    print(\"Starting ALRA denoising (Adaptively-thresholded Low Rank Approximation)...\")\n",
    "       \n",
    "    alra_data = adata_hvg.copy()\n",
    "\n",
    "    print(f\"ALRA data shape: {alra_data.shape}\")\n",
    "    print(f\"Data type: {type(alra_data.X)}\")\n",
    "    print(f\"Data range: {alra_data.X.min():.1f} - {alra_data.X.max():.1f}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        print(\"Applying low-rank approximation with adaptive thresholding...\")\n",
    "        X = alra_data.X.toarray() if hasattr(alra_data.X, 'toarray') else alra_data.X\n",
    "\n",
    "        n_components = min(100, min(X.shape) - 1)\n",
    "\n",
    "        svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "        X_transformed = svd.fit_transform(X)\n",
    "            \n",
    "        # Calculate explained variance ratio\n",
    "        explained_variance = svd.explained_variance_\n",
    "        explained_variance_ratio = svd.explained_variance_ratio_\n",
    "        cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "        variance_threshold = 0.90\n",
    "        optimal_components = np.where(cumulative_variance >= variance_threshold)[0]\n",
    "            \n",
    "        if len(optimal_components) > 0:\n",
    "            optimal_k = optimal_components[0] + 1\n",
    "            optimal_k = max(optimal_k, 15)  # ALRA recommends at least 10-20 components\n",
    "            print(f\"Adaptive rank selection: k={optimal_k} (explaining {cumulative_variance[optimal_k-1]:.1%} variance)\")\n",
    "        else:\n",
    "            optimal_k = max(20, n_components // 2)\n",
    "            print(f\"Using default rank: k={optimal_k}\")\n",
    "            \n",
    "        # Step 2: Recompute SVD with optimal rank\n",
    "        svd_optimal = TruncatedSVD(n_components=optimal_k, random_state=42)\n",
    "        X_transformed = svd_optimal.fit_transform(X)\n",
    "        X_reconstructed = svd_optimal.inverse_transform(X_transformed)\n",
    "            \n",
    "        # Step 3: Adaptive thresholding (key ALRA step)\n",
    "        print(\"Applying adaptive thresholding...\")\n",
    "\n",
    "        threshold = 0.1  # Threshold for considering a value as \"zero\" in log-space\n",
    "        imputation_mask = (X < threshold)\n",
    "\n",
    "        X_alra = X.copy()\n",
    "        X_alra[imputation_mask] = X_reconstructed[imputation_mask]\n",
    "            \n",
    "        # Ensure non-negative values\n",
    "        X_alra = np.maximum(X_alra, 0)\n",
    "            \n",
    "        # Calculate imputation statistics\n",
    "        n_imputed = np.sum(imputation_mask)\n",
    "        n_total = imputation_mask.size\n",
    "        imputation_rate = n_imputed / n_total * 100\n",
    "        print(f\"Imputed {n_imputed:,} / {n_total:,} values ({imputation_rate:.1f}%)\")\n",
    "            \n",
    "        # Update AnnData object\n",
    "        alra_data.X = X_alra\n",
    "\n",
    "        # Re-scale and PCA\n",
    "        sc.pp.scale(alra_data, max_value=10)\n",
    "        sc.tl.pca(alra_data, n_comps=50)\n",
    "\n",
    "        # Store ALRA embedding\n",
    "        alra_data.obsm[\"X_alra\"] = alra_data.obsm.get(\"X_pca\")\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"ALRA denoising completed, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "        print(f\"Final rank: k={optimal_k}, variance explained: {cumulative_variance[optimal_k-1]:.1%}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ALRA denoising failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        alra_data = None\n",
    "\n",
    "else:\n",
    "    print(\"Skipping ALRA denoising\")\n",
    "    alra_data = None\n",
    "\n",
    "# 3.3 DCA\n",
    "dca_data = None\n",
    "if DCA_AVAILABLE:\n",
    "    print(\"\\n3.3 DCA Denoising\")\n",
    "    print(\"Starting DCA denoising...\")\n",
    "        \n",
    "    # Create DCA data copy\n",
    "    dca_data = adata_hvg.copy()\n",
    "        \n",
    "    # DCA works best with raw counts\n",
    "    if dca_data.raw is not None:\n",
    "        print(\"Using raw counts from .raw layer\")\n",
    "        dca_input = sc.AnnData(\n",
    "            X=dca_data.raw.X,\n",
    "            obs=dca_data.obs,\n",
    "            var=dca_data.raw.var\n",
    "        )\n",
    "    else:\n",
    "        print(\"Note: No raw counts available, using current data\")\n",
    "        dca_input = dca_data.copy()\n",
    "        \n",
    "    print(f\"DCA input shape: {dca_input.shape}\")\n",
    "    start_time = time.time()\n",
    "        \n",
    "    try:\n",
    "        # Apply DCA denoising\n",
    "        print(\"Running DCA with ZINB autoencoder...\")\n",
    "        dca_denoise(\n",
    "            dca_input,\n",
    "            mode='denoise',\n",
    "            ae_type='zinb-conddisp',\n",
    "            normalize_per_cell=True,\n",
    "            scale=True,\n",
    "            log1p=True,\n",
    "            hidden_size=[64, 32, 64],\n",
    "            epochs=300,\n",
    "            learning_rate=0.001,\n",
    "            verbose=True\n",
    "        )\n",
    "            \n",
    "        # Copy denoised data\n",
    "        dca_data.X = dca_input.X\n",
    "            \n",
    "        # Compute PCA\n",
    "        sc.pp.scale(dca_data, max_value=10)\n",
    "        sc.tl.pca(dca_data, n_comps=50)\n",
    "        dca_data.obsm[\"X_dca\"] = dca_data.obsm.get(\"X_pca\")\n",
    "            \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"DCA denoising completed, time taken: {elapsed_time:.2f} seconds\")\n",
    "        print(f\"DCA data shape: {dca_data.shape}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"DCA denoising failed: {e}\")\n",
    "        print(\"This is expected due to Keras compatibility issues\")\n",
    "        dca_data = None\n",
    "    \n",
    "else:\n",
    "    print(\"\\nSkipping DCA denoising (library not available or incompatible)\")\n",
    "    dca_data = None\n",
    "\n",
    "# 3.4 kNN-smoothing Denoising\n",
    "knn_data = None\n",
    "if KNN_AVAILABLE:\n",
    "    print(\"\\n3.4 kNN-smoothing Denoising\")\n",
    "    print(\"Starting kNN-smoothing denoising...\")\n",
    "        \n",
    "    # Create kNN data copy\n",
    "    knn_data = adata_hvg.copy()\n",
    "        \n",
    "    print(f\"kNN input shape: {knn_data.shape}\")\n",
    "    start_time = time.time()\n",
    "        \n",
    "    try:\n",
    "        # Get data as array\n",
    "        if hasattr(knn_data.X, 'toarray'):\n",
    "            X = knn_data.X.toarray()\n",
    "        else:\n",
    "            X = knn_data.X.copy()\n",
    "            \n",
    "        # Set k-nearest neighbors\n",
    "        n_neighbors = 15\n",
    "        print(f\"Using {n_neighbors} nearest neighbors for smoothing...\")\n",
    "            \n",
    "        # Find k nearest neighbors\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors, metric='euclidean', n_jobs=-1)\n",
    "        nbrs.fit(X)\n",
    "        distances, indices = nbrs.kneighbors(X)\n",
    "            \n",
    "        # Weight by inverse distance\n",
    "        epsilon = 1e-10  # Avoid division by zero\n",
    "        weights = 1.0 / (distances + epsilon)\n",
    "        weights = weights / weights.sum(axis=1, keepdims=True)\n",
    "            \n",
    "        # Apply weighted averaging\n",
    "        print(\"Applying weighted k-nearest neighbor averaging...\")\n",
    "        X_smoothed = np.zeros_like(X)\n",
    "        for i in range(len(X)):\n",
    "            neighbor_expr = X[indices[i]]\n",
    "            X_smoothed[i] = (neighbor_expr * weights[i][:, np.newaxis]).sum(axis=0)\n",
    "            \n",
    "        # Store smoothed data\n",
    "        knn_data.X = X_smoothed\n",
    "            \n",
    "        # Compute PCA\n",
    "        sc.pp.scale(knn_data, max_value=10)\n",
    "        sc.tl.pca(knn_data, n_comps=50)\n",
    "        knn_data.obsm[\"X_knn\"] = knn_data.obsm.get(\"X_pca\")\n",
    "            \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"kNN-smoothing completed, time taken: {elapsed_time:.2f} seconds\")\n",
    "        print(f\"kNN data shape: {knn_data.shape}\")\n",
    "        print(f\"Smoothing strength: {n_neighbors} neighbors\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"kNN-smoothing failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        knn_data = None\n",
    "    \n",
    "else:\n",
    "    print(\"\\nSkipping kNN-smoothing\")\n",
    "    knn_data = None\n",
    "\n",
    "# Save results\n",
    "print(\"\\nSaving results...\")\n",
    "adata_hvg.write_h5ad(\"results/baseline_denoised.h5ad\")\n",
    "if magic_data is not None:\n",
    "    magic_data.write_h5ad(\"results/magic_denoised.h5ad\")\n",
    "    print(\"Saved: results/magic_denoised.h5ad\")\n",
    "if alra_data is not None:\n",
    "    alra_data.write_h5ad(\"results/alra_denoised.h5ad\")\n",
    "    print(\"Saved: results/alra_denoised.h5ad\")\n",
    "if dca_data is not None:\n",
    "    dca_data.write_h5ad(\"results/dca_denoised.h5ad\")\n",
    "    print(\"Saved: results/dca_denoised.h5ad\")\n",
    "if knn_data is not None:\n",
    "    knn_data.write_h5ad(\"results/knn_denoised.h5ad\")\n",
    "    print(\"Saved: results/knn_denoised.h5ad\")\n",
    "\n",
    "print(\"\\nDenoising analysis completed!\")\n",
    "total_methods = sum([magic_data is not None, alra_data is not None, dca_data is not None, knn_data is not None])\n",
    "print(f\"Successfully denoised datasets: {total_methods} / 4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Batch Correction & Clustering\n",
    "\n",
    "**Functions:**\n",
    "- BBKNN batch correction\n",
    "- Scanorama batch correction\n",
    "- Harmony batch correction\n",
    "- Leiden clustering\n",
    "- K-means clustering\n",
    "- UMAP visualization\n",
    "\n",
    "**Outputs:**\n",
    "- `results/bbknn_corrected.h5ad`\n",
    "- `results/scanorama_corrected.h5ad`\n",
    "- `results/harmony_corrected.h5ad`\n",
    "- `figures/umap_*.png` (multiple UMAP plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_clusters(adata_obj, use_rep=None, suffix=\"\"):\n",
    "    \"\"\"Add multiple clustering results to AnnData object\"\"\"\n",
    "    # Compute neighbor graph for graph-based clustering\n",
    "    sc.pp.neighbors(adata_obj, use_rep=use_rep, n_neighbors=15)\n",
    "    \n",
    "    # Leiden clustering\n",
    "    sc.tl.leiden(adata_obj, resolution=1.0, key_added=f\"leiden{suffix}\")\n",
    "    \n",
    "    # Louvain clustering\n",
    "    try:\n",
    "        sc.tl.louvain(adata_obj, resolution=1.0, key_added=f\"louvain{suffix}\")\n",
    "    except (ImportError, ModuleNotFoundError) as e:\n",
    "        print(f\"Warning: Louvain clustering skipped ({e}). Using Leiden only (Leiden is an improved version of Louvain).\")\n",
    "\n",
    "    # K-means clustering\n",
    "    emb = adata_obj.obsm.get(\"X_pca\") if use_rep is None else adata_obj.obsm.get(use_rep)\n",
    "    if emb is not None:\n",
    "        km = KMeans(n_clusters=12, random_state=7, n_init=\"auto\").fit_predict(emb[:, :30])\n",
    "        adata_obj.obs[f\"kmeans{suffix}\"] = km.astype(str)\n",
    "\n",
    "def compute_metrics(adata_obj, cluster_key, embedding_key=\"X_pca\"):\n",
    "    \"\"\"Calculate clustering evaluation metrics\"\"\"\n",
    "    if embedding_key not in adata_obj.obsm:\n",
    "        embedding_key = \"X_pca\"\n",
    "    emb = adata_obj.obsm.get(embedding_key)\n",
    "    labels = adata_obj.obs[cluster_key].astype(str)\n",
    "\n",
    "    if emb is None or labels.nunique() < 2:\n",
    "        return {\"silhouette\": np.nan, \"ari\": np.nan, \"rand\": np.nan}\n",
    "\n",
    "    try:\n",
    "        sil = float(silhouette_score(emb, labels))\n",
    "    except ValueError:\n",
    "        sil = np.nan\n",
    "\n",
    "    if \"cluster\" in adata_obj.obs:\n",
    "        ref = adata_obj.obs[\"cluster\"].astype(str)\n",
    "        ari = float(adjusted_rand_score(ref, labels))\n",
    "        rand = float(rand_score(ref, labels))\n",
    "    else:\n",
    "        ari = np.nan\n",
    "        rand = np.nan\n",
    "\n",
    "    return {\"silhouette\": sil, \"ari\": ari, \"rand\": rand}\n",
    "\n",
    "# Create necessary directories\n",
    "Path(\"results\").mkdir(exist_ok=True)\n",
    "Path(\"figures\").mkdir(exist_ok=True)\n",
    "\n",
    "# Check if denoised data exists\n",
    "baseline_file = Path(\"results/baseline_denoised.h5ad\")\n",
    "if not baseline_file.exists():\n",
    "    print(\"Error: run 02_data_denoising.py first\")\n",
    "    raise FileNotFoundError(\"Run Part 2: Data Denoising first\")\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "datasets = {}\n",
    "\n",
    "# Load baseline data\n",
    "adata_hvg = sc.read_h5ad(baseline_file)\n",
    "datasets[\"\"] = (adata_hvg, None)\n",
    "\n",
    "# Load MAGIC data (if exists)\n",
    "magic_file = Path(\"results/magic_denoised.h5ad\")\n",
    "if magic_file.exists():\n",
    "    datasets[\"magic\"] = (sc.read_h5ad(magic_file), None)\n",
    "\n",
    "# Load ALRA data (if exists)\n",
    "alra_file = Path(\"results/alra_denoised.h5ad\")\n",
    "if alra_file.exists():\n",
    "    datasets[\"alra\"] = (sc.read_h5ad(alra_file), \"X_alra\")\n",
    "\n",
    "# Check batch information\n",
    "batch_key = 'DaysPostAmputation' if 'DaysPostAmputation' in adata_hvg.obs.columns else None\n",
    "if batch_key:\n",
    "    print(f\"Using batch key: {batch_key}\")\n",
    "    print(f\"Batch distribution: {adata_hvg.obs[batch_key].value_counts()}\")\n",
    "else:\n",
    "    print(\"No batch information found\")\n",
    "\n",
    "# 3.1 Check batch correction library availability\n",
    "print(\"\\n3.1 Check batch correction library availability\")\n",
    "\n",
    "try:\n",
    "    from scanpy.external.pp import bbknn\n",
    "    BBKNN_AVAILABLE = True\n",
    "    print(\"[OK] BBKNN available\")\n",
    "except ImportError:\n",
    "    BBKNN_AVAILABLE = False\n",
    "    print(\"[FAIL] BBKNN unavailable\")\n",
    "\n",
    "try:\n",
    "    from scanpy.external.pp import harmony_integrate\n",
    "    HARMONY_AVAILABLE = True\n",
    "    print(\"[OK] Harmony available\")\n",
    "except ImportError:\n",
    "    HARMONY_AVAILABLE = False\n",
    "    print(\"[FAIL] Harmony unavailable\")\n",
    "\n",
    "try:\n",
    "    import scanorama\n",
    "    SCANORAMA_AVAILABLE = True\n",
    "    print(\"[OK] Scanorama available\")\n",
    "except ImportError:\n",
    "    SCANORAMA_AVAILABLE = False\n",
    "    print(\"[FAIL] Scanorama unavailable\")\n",
    "\n",
    "# 3.2 BBKNN Batch Correction\n",
    "print(\"\\n3.2 BBKNN Batch Correction\")\n",
    "\n",
    "bbknn_data = None\n",
    "if BBKNN_AVAILABLE and batch_key:\n",
    "    # Create BBKNN data copy\n",
    "    bbknn_data = adata_hvg.copy()\n",
    "\n",
    "    print(\"Starting BBKNN batch correction...\")\n",
    "    print(\"Note: BBKNN may be slow on large datasets. Consider using Harmony instead.\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Apply BBKNN with optimized parameters\n",
    "        bbknn(bbknn_data, batch_key=batch_key, neighbors_within_batch=3, n_pcs=20)\n",
    "\n",
    "        # Calculate UMAP\n",
    "        sc.tl.umap(bbknn_data, random_state=0, n_components=2, maxiter=100)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"BBKNN batch correction completed, time taken: {elapsed_time:.2f} seconds\")\n",
    "        datasets[\"bbknn\"] = (bbknn_data, None)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"BBKNN interrupted by user, skipping...\")\n",
    "        bbknn_data = None\n",
    "    except Exception as e:\n",
    "        print(f\"BBKNN batch correction failed: {e}\")\n",
    "        bbknn_data = None\n",
    "else:\n",
    "    print(\"Skipping BBKNN batch correction\")\n",
    "\n",
    "# 3.3 Scanorama Batch Correction\n",
    "print(\"\\n3.3 Scanorama Batch Correction\")\n",
    "\n",
    "corrected_scanorama = None\n",
    "if SCANORAMA_AVAILABLE and batch_key:\n",
    "    try:\n",
    "        from scanpy.external.pp import scanorama_integrate\n",
    "            \n",
    "        # Create Scanorama data copy and sort by batch (required for scanorama_integrate)\n",
    "        scanorama_data = adata_hvg.copy()\n",
    "        scanorama_data = scanorama_data[scanorama_data.obs[batch_key].argsort()].copy()\n",
    "\n",
    "        print(\"Starting Scanorama batch correction...\")\n",
    "        print(f\"Data shape: {scanorama_data.shape}\")\n",
    "        print(f\"Number of batches: {scanorama_data.obs[batch_key].nunique()}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Use scanpy's scanorama_integrate\n",
    "        # This method is much faster and more stable than scanorama.correct()\n",
    "        scanorama_integrate(scanorama_data, key=batch_key, basis='X_pca', adjusted_basis='X_scanorama')\n",
    "            \n",
    "        corrected_scanorama = scanorama_data\n",
    "            \n",
    "        # Calculate UMAP using the integrated representation\n",
    "        sc.pp.neighbors(corrected_scanorama, n_neighbors=10, use_rep='X_scanorama')\n",
    "        sc.tl.umap(corrected_scanorama, random_state=0, n_components=2, maxiter=100)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Scanorama batch correction completed, time taken: {elapsed_time:.2f} seconds\")\n",
    "        datasets[\"scanorama\"] = (corrected_scanorama, 'X_scanorama')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Scanorama batch correction failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        corrected_scanorama = None\n",
    "else:\n",
    "    print(\"Skipping Scanorama batch correction\")\n",
    "\n",
    "# 3.4 Simplified Batch Correction (Linear Regression)\n",
    "print(\"\\n3.4 Simplified Batch Correction (Linear Regression)\")\n",
    "    \n",
    "linear_corrected = None\n",
    "if batch_key:\n",
    "    print(\"Starting simplified batch correction (linear regression)...\")\n",
    "    #print(\"Note: This is a basic method that corrects batch effects using linear regression.\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "\n",
    "        linear_corrected = adata_hvg.copy()\n",
    "        X = linear_corrected.X.toarray() if hasattr(linear_corrected.X, 'toarray') else linear_corrected.X\n",
    "\n",
    "        print(f\"Correcting batch effects for {X.shape[1]} genes...\")\n",
    "            \n",
    "        # Correct batch effects for each gene\n",
    "        batch_indicators = pd.get_dummies(linear_corrected.obs[batch_key]).values\n",
    "            \n",
    "        # Use first batch as reference\n",
    "        reference_batch = batch_indicators[:, 0] == 1\n",
    "\n",
    "        X_corrected = np.zeros_like(X)\n",
    "\n",
    "        # Correct all genes\n",
    "        for gene_idx in range(X.shape[1]):\n",
    "            if gene_idx % 500 == 0:\n",
    "                print(f\"  Progress: {gene_idx}/{X.shape[1]} genes corrected\")\n",
    "                \n",
    "            gene_expr = X[:, gene_idx]\n",
    "\n",
    "            # Build linear model: gene expression = batch effect + residual\n",
    "            lr = LinearRegression(fit_intercept=True)\n",
    "            lr.fit(batch_indicators, gene_expr)\n",
    "\n",
    "            # Remove batch effects (preserve reference batch level)\n",
    "            predicted_batch_effects = lr.predict(batch_indicators)\n",
    "            reference_mean = np.mean(gene_expr[reference_batch]) if np.sum(reference_batch) > 0 else 0\n",
    "\n",
    "            X_corrected[:, gene_idx] = gene_expr - predicted_batch_effects + reference_mean\n",
    "\n",
    "        linear_corrected.X = X_corrected\n",
    "\n",
    "        # Recalculate PCA and UMAP\n",
    "        print(\"Recalculating PCA and UMAP...\")\n",
    "        sc.pp.scale(linear_corrected, max_value=10)\n",
    "        sc.tl.pca(linear_corrected, n_comps=50)\n",
    "        sc.pp.neighbors(linear_corrected, n_neighbors=10, n_pcs=20)\n",
    "        sc.tl.umap(linear_corrected, random_state=0, n_components=2, maxiter=100)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Simplified batch correction completed, time taken: {elapsed_time:.2f} seconds\")\n",
    "        datasets[\"linear\"] = (linear_corrected, None)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Simplified batch correction failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        linear_corrected = None\n",
    "else:\n",
    "    print(\"Skipping simplified batch correction (no batch information)\")\n",
    "\n",
    "# 3.5 Harmony Batch Correction\n",
    "print(\"\\n3.5 Harmony Batch Correction\")\n",
    "\n",
    "harmony_data = None\n",
    "if HARMONY_AVAILABLE and batch_key:\n",
    "    # Create Harmony data copy\n",
    "    harmony_data = adata_hvg.copy()\n",
    "\n",
    "    print(\"Starting Harmony batch correction...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # IMPORTANT: Harmony requires batch key to be string or categorical, not int\n",
    "        # Convert batch key to string to avoid 'unique' error\n",
    "        harmony_data.obs[batch_key] = harmony_data.obs[batch_key].astype(str)\n",
    "        print(f\"Converted {batch_key} to string type for Harmony compatibility\")\n",
    "            \n",
    "        # Apply Harmony batch correction\n",
    "        harmony_integrate(harmony_data, key=batch_key, max_iter_harmony=20)\n",
    "\n",
    "        # Calculate UMAP\n",
    "        sc.pp.neighbors(harmony_data, n_neighbors=10, n_pcs=20)\n",
    "        sc.tl.umap(harmony_data, random_state=0, n_components=2, maxiter=100)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Harmony batch correction completed, time taken: {elapsed_time:.2f} seconds\")\n",
    "        datasets[\"harmony\"] = (harmony_data, None)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Harmony batch correction failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        harmony_data = None\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Harmony batch correction\")\n",
    "\n",
    "# 3.6 Execute Clustering Analysis\n",
    "print(\"\\n3.6 Execute Clustering Analysis\")\n",
    "\n",
    "for suffix, (dataset, rep) in datasets.items():\n",
    "    print(f\"Executing clustering on {suffix or 'baseline'} dataset...\")\n",
    "    add_clusters(dataset, use_rep=rep, suffix=suffix)\n",
    "\n",
    "# 3.7 Calculate clustering evaluation metrics\n",
    "print(\"\\n3.7 Calculate clustering evaluation metrics\")\n",
    "\n",
    "clustering_results = []\n",
    "for suffix, (dataset, rep) in datasets.items():\n",
    "    dataset_name = suffix or \"baseline\"\n",
    "    embedding_key = rep or \"X_pca\"\n",
    "\n",
    "    for method in [\"kmeans\"]:\n",
    "        cluster_key = f\"{method}{suffix}\"\n",
    "        if cluster_key in dataset.obs:\n",
    "            metrics = compute_metrics(dataset, cluster_key, embedding_key)\n",
    "            clustering_results.append({\n",
    "                \"dataset\": dataset_name,\n",
    "                \"method\": method,\n",
    "                \"silhouette\": metrics[\"silhouette\"],\n",
    "                \"ari\": metrics[\"ari\"],\n",
    "                \"rand\": metrics[\"rand\"],\n",
    "                \"n_clusters\": dataset.obs[cluster_key].nunique()\n",
    "            })\n",
    "\n",
    "clustering_df = pd.DataFrame(clustering_results)\n",
    "print(\"Clustering evaluation results:\")\n",
    "print(clustering_df)\n",
    "\n",
    "# 3.8 UMAP Visualization (Reproduce Figure 1B)\n",
    "print(\"\\n3.8 UMAP Visualization\")\n",
    "\n",
    "# Calculate UMAP for baseline dataset\n",
    "baseline_dataset = adata_hvg\n",
    "sc.pp.neighbors(baseline_dataset, n_neighbors=10, n_pcs=20)\n",
    "sc.tl.umap(baseline_dataset, random_state=0, n_components=2, maxiter=100)\n",
    "\n",
    "# Create clustering comparison visualization (only show baseline)\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "sc.pl.umap(baseline_dataset, color=\"kmeans\", show=False, ax=ax,\n",
    "          legend_loc=\"on data\", size=10, title=\"K-means Clustering (Baseline)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/umap_clustering_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"UMAP clustering comparison plot saved\")\n",
    "\n",
    "# 3.8.1 Create comprehensive UMAP clustering metrics comparison\n",
    "print(\"\\n3.8.1 Create comprehensive UMAP clustering metrics comparison\")\n",
    "\n",
    "if len(datasets) > 1:\n",
    "    # Calculate UMAP for all datasets that don't have it\n",
    "    for suffix, (dataset, rep) in datasets.items():\n",
    "        if 'X_umap' not in dataset.obsm:\n",
    "            try:\n",
    "                sc.pp.neighbors(dataset, n_neighbors=10, n_pcs=20, use_rep=rep)\n",
    "                sc.tl.umap(dataset, random_state=0, n_components=2, maxiter=100)\n",
    "                print(f\"Calculated UMAP for {suffix or 'baseline'} dataset\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to calculate UMAP for {suffix or 'baseline'} dataset: {e}\")\n",
    "\n",
    "    # Create a figure comparing different batch correction methods\n",
    "    n_methods = len(datasets)\n",
    "    fig, axes = plt.subplots(1, n_methods, figsize=(6*n_methods, 5))\n",
    "\n",
    "    if n_methods == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, (suffix, (dataset, rep)) in enumerate(datasets.items()):\n",
    "        method_name = suffix or \"baseline\"\n",
    "        cluster_key = f\"kmeans{suffix}\"\n",
    "\n",
    "        if cluster_key in dataset.obs and 'X_umap' in dataset.obsm:\n",
    "            sc.pl.umap(dataset, color=cluster_key, show=False, ax=axes[i],\n",
    "                      legend_loc=\"on data\" if i == 0 else None,\n",
    "                      size=8, title=f\"{method_name.upper()} Clustering\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"figures/umap_clustering_metrics_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"UMAP clustering metrics comparison plot saved\")\n",
    "else:\n",
    "    print(\"Skipping clustering metrics comparison (only one dataset available)\")\n",
    "\n",
    "# 3.8.2 Create UMAP with time point coloring (Figure 1B replication)\n",
    "print(\"\\n3.8.2 Create UMAP with time point coloring (Figure 1B replication)\")\n",
    "\n",
    "# Use the batch key (DaysPostAmputation) as time information\n",
    "time_column = batch_key or 'time'\n",
    "if time_column and time_column in baseline_dataset.obs.columns:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "    # Color by time points with colorful palette\n",
    "    sc.pl.umap(baseline_dataset, color=time_column, show=False, ax=ax,\n",
    "              size=10, title=\"UMAP by Time Points (Figure 1B Replication)\",\n",
    "              palette='viridis', cmap='viridis')  # Add colorful colormap\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"figures/umap_umap_figure_1b_replication.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"UMAP Figure 1B replication plot saved\")\n",
    "else:\n",
    "    print(\"Skipping Figure 1B replication (no time information)\")\n",
    "\n",
    "# 3.8.3 Create UMAP showing marker gene expression\n",
    "print(\"\\n3.8.3 Create UMAP showing marker gene expression\")\n",
    "\n",
    "# Identify top marker genes for the largest cluster\n",
    "if \"kmeans\" in baseline_dataset.obs:\n",
    "    # Find the largest cluster\n",
    "    largest_cluster = baseline_dataset.obs[\"kmeans\"].value_counts().index[0]\n",
    "\n",
    "    # Get marker genes for this cluster\n",
    "    try:\n",
    "        sc.tl.rank_genes_groups(baseline_dataset, \"kmeans\", method=\"wilcoxon\",\n",
    "                               key_added=\"marker_genes\")\n",
    "\n",
    "        # Get top 6 marker genes\n",
    "        top_markers = []\n",
    "        for i in range(min(6, len(baseline_dataset.uns[\"marker_genes\"][\"names\"][largest_cluster]))):\n",
    "            gene = baseline_dataset.uns[\"marker_genes\"][\"names\"][largest_cluster][i]\n",
    "            if gene in baseline_dataset.var_names:\n",
    "                top_markers.append(gene)\n",
    "\n",
    "        if top_markers:\n",
    "            # Create UMAP plots for top marker genes\n",
    "            n_markers = len(top_markers)\n",
    "            n_cols = min(3, n_markers)\n",
    "            n_rows = (n_markers + n_cols - 1) // n_cols\n",
    "\n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "\n",
    "            if n_rows == 1 and n_cols == 1:\n",
    "                axes = [axes]\n",
    "            elif n_rows == 1:\n",
    "                axes = axes.flatten()\n",
    "            else:\n",
    "                axes = axes.flatten()\n",
    "\n",
    "            for i, gene in enumerate(top_markers):\n",
    "                if i < len(axes):\n",
    "                    sc.pl.umap(baseline_dataset, color=gene, show=False, ax=axes[i],\n",
    "                              size=8, title=f\"{gene} Expression\", cmap=\"viridis\")\n",
    "\n",
    "            # Hide unused subplots\n",
    "            for i in range(len(top_markers), len(axes)):\n",
    "                axes[i].set_visible(False)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\"figures/umap_marker_gene_expression.png\", dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(f\"UMAP marker gene expression plot saved ({len(top_markers)} genes)\")\n",
    "        else:\n",
    "            print(\"No valid marker genes found for expression visualization\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create marker gene expression plot: {e}\")\n",
    "else:\n",
    "    print(\"Skipping marker gene expression plot (no clustering available)\")\n",
    "\n",
    "# 3.9 Save Results\n",
    "print(\"\\n3.9 Save Results\")\n",
    "\n",
    "# Save clustering metrics\n",
    "clustering_df.to_csv(\"results/clustering_metrics.csv\", index=False)\n",
    "\n",
    "# Save batch-corrected data\n",
    "for suffix, (dataset, rep) in datasets.items():\n",
    "    if suffix:  # Only save batch-corrected data\n",
    "        dataset.write_h5ad(f\"results/{suffix}_corrected.h5ad\")\n",
    "\n",
    "# Save baseline dataset with clustering results\n",
    "baseline_dataset.write_h5ad(\"results/baseline_clustered.h5ad\")\n",
    "\n",
    "print(\"All completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Biomarker Analysis\n",
    "\n",
    "**Functions:**\n",
    "- ROC cell identification (Leiden + K-means automatic selection)\n",
    "- Differential expression gene analysis\n",
    "- Cross-validation (Logistic vs Wilcoxon)\n",
    "\n",
    "**Outputs:**\n",
    "- `results/roc_markers_wilcoxon.csv`\n",
    "- `results/roc_markers_logistic.csv`\n",
    "- `results/roc_markers_common.csv`\n",
    "- `figures/roc_*.png` (multiple ROC-related plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Biomarker Identification and Functional Analysis\")\n",
    "\n",
    "# Create necessary directories\n",
    "Path(\"results\").mkdir(exist_ok=True)\n",
    "Path(\"figures\").mkdir(exist_ok=True)\n",
    "\n",
    "# Check if clustering data exists\n",
    "baseline_file = Path(\"results/baseline_denoised.h5ad\")\n",
    "if not baseline_file.exists():\n",
    "    print(\"Error: Clustering data file not found, please run 03_batch_correction_clustering.py first\")\n",
    "    raise FileNotFoundError(\"Run Part 3: Batch Correction & Clustering first\")\n",
    "\n",
    "# Load data (use clustered baseline data)\n",
    "print(\"Loading data...\")\n",
    "baseline_file = \"results/baseline_clustered.h5ad\"\n",
    "if not os.path.exists(baseline_file):\n",
    "    print(f\"Warning: {baseline_file} not found, using preprocessed data\")\n",
    "    baseline_file = \"results/preprocessed_data.h5ad\"\n",
    "\n",
    "adata_hvg = sc.read_h5ad(baseline_file)\n",
    "\n",
    "# Check batch information\n",
    "batch_key = 'DaysPostAmputation' if 'DaysPostAmputation' in adata_hvg.obs.columns else None\n",
    "\n",
    "# 4.1 Identify potential ROC clusters (with leiden+kmeans support)\n",
    "print(\"\\n4.1 Identify potential ROC clusters\")\n",
    "\n",
    "potential_roc_clusters = {}\n",
    "\n",
    "# Support both leiden and kmeans - automatically select the best one\n",
    "for method in [\"leiden\", \"kmeans\"]:\n",
    "    cluster_key = f\"{method}\"\n",
    "    if cluster_key in adata_hvg.obs:\n",
    "        print(f\"Analyzing {method.upper()} clustering results:\")\n",
    "\n",
    "        if batch_key and batch_key in adata_hvg.obs.columns:\n",
    "            cluster_time_dist = pd.crosstab(adata_hvg.obs[cluster_key], adata_hvg.obs[batch_key], normalize='index')\n",
    "            early_timepoints = cluster_time_dist.columns[:2]\n",
    "            early_enrichment = cluster_time_dist[early_timepoints].sum(axis=1)\n",
    "\n",
    "            roc_candidates = early_enrichment[early_enrichment > early_enrichment.mean() + early_enrichment.std()].index.tolist()\n",
    "\n",
    "            if roc_candidates:\n",
    "                potential_roc_clusters[f\"baseline_{method}\"] = roc_candidates\n",
    "                print(f\"Potential ROC clusters: {roc_candidates}\")\n",
    "                print(f\"Early time point enrichment: {early_enrichment[roc_candidates].values}\")\n",
    "            else:\n",
    "                print(\"No significantly enriched clusters found\")\n",
    "\n",
    "print(f\"Potential ROC clusters summary: {potential_roc_clusters}\")\n",
    "\n",
    "# Select the best ROC cluster (highest early enrichment)\n",
    "best_roc_key = None\n",
    "best_enrichment = 0\n",
    "best_roc_cluster = None\n",
    "\n",
    "if potential_roc_clusters:\n",
    "    for roc_key, clusters in potential_roc_clusters.items():\n",
    "        dataset_name, method = roc_key.rsplit('_', 1)\n",
    "        cluster_key = f\"{method}\"\n",
    "            \n",
    "        if cluster_key in adata_hvg.obs and batch_key in adata_hvg.obs.columns:\n",
    "            cluster_time_dist = pd.crosstab(adata_hvg.obs[cluster_key], adata_hvg.obs[batch_key], normalize='index')\n",
    "            early_timepoints = cluster_time_dist.columns[:2]\n",
    "            early_enrichment = cluster_time_dist[early_timepoints].sum(axis=1)\n",
    "                \n",
    "            for cluster in clusters:\n",
    "                enrichment = early_enrichment.loc[cluster]\n",
    "                if enrichment > best_enrichment:\n",
    "                    best_enrichment = enrichment\n",
    "                    best_roc_key = roc_key\n",
    "                    best_roc_cluster = cluster\n",
    "        \n",
    "    if best_roc_key:\n",
    "        print(f\"   Best ROC cluster selected: {best_roc_cluster} from {best_roc_key}\")\n",
    "        print(f\"   Early enrichment: {best_enrichment:.1%}\")\n",
    "\n",
    "# 4.2 Marker gene identification\n",
    "print(\"\\n4.2 Marker gene identification\")\n",
    "\n",
    "cluster_key = \"kmeans\"  # Default to kmeans clustering\n",
    "\n",
    "def find_markers_logistic_regression(adata, cluster_key, cluster_of_interest):\n",
    "    # Identify marker genes using logistic regression\n",
    "    # Prepare data\n",
    "    X = adata.X.toarray() if hasattr(adata.X, 'toarray') else adata.X\n",
    "    y = (adata.obs[cluster_key] == cluster_of_interest).astype(int)\n",
    "\n",
    "    # Train logistic regression model\n",
    "    clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Get feature importance\n",
    "    feature_importance = np.abs(clf.coef_[0])\n",
    "\n",
    "    # Create results DataFrame\n",
    "    markers_df = pd.DataFrame({\n",
    "        'gene': adata.var_names,\n",
    "        'importance': feature_importance\n",
    "    })\n",
    "\n",
    "    # Sort by importance\n",
    "    markers_df = markers_df.sort_values('importance', ascending=False)\n",
    "\n",
    "    return markers_df\n",
    "\n",
    "def find_markers_wilcoxon(adata, cluster_key, cluster_of_interest):\n",
    "    # Identify marker genes using Wilcoxon rank-sum test\n",
    "    cluster_mask = adata.obs[cluster_key] == cluster_of_interest\n",
    "    other_mask = adata.obs[cluster_key] != cluster_of_interest\n",
    "\n",
    "    p_values = []\n",
    "    log_fold_changes = []\n",
    "\n",
    "    X = adata.X.toarray() if hasattr(adata.X, 'toarray') else adata.X\n",
    "\n",
    "    for i in range(X.shape[1]):\n",
    "        cluster_expr = X[cluster_mask, i]\n",
    "        other_expr = X[other_mask, i]\n",
    "\n",
    "        # Calculate log fold change\n",
    "        cluster_mean = np.mean(cluster_expr)\n",
    "        other_mean = np.mean(other_expr)\n",
    "        lfc = np.log2(cluster_mean + 1) - np.log2(other_mean + 1)\n",
    "        log_fold_changes.append(lfc)\n",
    "\n",
    "        # Wilcoxon rank-sum test\n",
    "        try:\n",
    "            stat, p_val = ranksums(cluster_expr, other_expr)\n",
    "            p_values.append(p_val)\n",
    "        except ValueError:\n",
    "            p_values.append(1.0)\n",
    "\n",
    "    # Create results DataFrame\n",
    "    markers_df = pd.DataFrame({\n",
    "        'gene': adata.var_names,\n",
    "        'p_value': p_values,\n",
    "        'log_fold_change': log_fold_changes,\n",
    "        'neg_log_p': -np.log10(np.array(p_values) + 1e-10)\n",
    "    })\n",
    "\n",
    "    # Sort by p-value\n",
    "    markers_df = markers_df.sort_values('p_value', ascending=True)\n",
    "\n",
    "    return markers_df\n",
    "\n",
    "if best_roc_cluster:\n",
    "    dataset_name, method = best_roc_key.rsplit('_', 1)\n",
    "    cluster_key = f\"{method}\"\n",
    "\n",
    "    if cluster_key in adata_hvg.obs:\n",
    "        roc_cluster = best_roc_cluster\n",
    "        print(f\"Selected ROC cluster: {roc_cluster} (from {best_roc_key})\")\n",
    "\n",
    "        # Logistic regression\n",
    "        print(\"Using Logistic regression to identify marker genes...\")\n",
    "        markers_logistic = find_markers_logistic_regression(adata_hvg, cluster_key, roc_cluster)\n",
    "        markers_logistic.to_csv(\"results/roc_markers_logistic.csv\", index=False)\n",
    "\n",
    "        # Wilcoxon test\n",
    "        print(\"Using Wilcoxon test to identify marker genes...\")\n",
    "        markers_wilcoxon = find_markers_wilcoxon(adata_hvg, cluster_key, roc_cluster)\n",
    "        markers_wilcoxon.to_csv(\"results/roc_markers_wilcoxon.csv\", index=False)\n",
    "\n",
    "        print(\"Logistic regression - top 10 marker genes:\")\n",
    "        print(markers_logistic.head(10))\n",
    "\n",
    "        print(\"Wilcoxon test - top 10 marker genes:\")\n",
    "        print(markers_wilcoxon.head(10))\n",
    "else:\n",
    "    print(\"No ROC clusters identified, proceeding with general marker gene analysis\")\n",
    "\n",
    "# 4.2.1 Marker gene visualization - Always run for kmeans\n",
    "print(\"\\n4.2.1 Marker gene visualization\")\n",
    "\n",
    "cluster_key = \"kmeans\"  # Use kmeans clustering for visualization\n",
    "if cluster_key in adata_hvg.obs:\n",
    "    # Compute dendrogram first to remove warning\n",
    "    try:\n",
    "        sc.tl.dendrogram(adata_hvg, groupby=cluster_key)\n",
    "    except:\n",
    "        pass  # Skip if dendrogram computation fails\n",
    "\n",
    "    # Compute marker genes using Wilcoxon test (more robust for sparse data)\n",
    "    print(\"Computing marker genes for visualization...\")\n",
    "    sc.tl.rank_genes_groups(\n",
    "        adata_hvg,\n",
    "        groupby=cluster_key,\n",
    "        method=\"wilcoxon\",\n",
    "        n_genes=30,\n",
    "        key_added=\"rank_genes_clusters\"\n",
    "    )\n",
    "\n",
    "    # Save marker genes results\n",
    "    marker_df = sc.get.rank_genes_groups_df(adata_hvg, group=None, key=\"rank_genes_clusters\")\n",
    "    marker_df.to_csv(\"results/marker_genes_all_clusters.csv\", index=False)\n",
    "    print(\"Marker genes saved to: results/marker_genes_all_clusters.csv\")\n",
    "\n",
    "    # Visualization - rank genes groups plot\n",
    "    print(\"Creating rank genes groups plot...\")\n",
    "    try:\n",
    "        sc.pl.rank_genes_groups(adata_hvg, key=\"rank_genes_clusters\", n_genes=5, sharey=False, show=False)\n",
    "        plt.savefig('figures/rank_genes_groups_plot.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"Rank genes groups plot saved to figures/rank_genes_groups_plot.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"Rank genes groups plot failed: {e}\")\n",
    "\n",
    "    # Visualization - dotplot (vertical orientation)\n",
    "    print(\"Creating dotplot...\")\n",
    "    try:\n",
    "        sc.pl.rank_genes_groups_dotplot(adata_hvg, key=\"rank_genes_clusters\", n_genes=5, swap_axes=True, show=False)\n",
    "        plt.savefig('figures/marker_genes_dotplot.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"Dotplot saved to figures/marker_genes_dotplot.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"Dotplot failed: {e}\")\n",
    "\n",
    "    # Visualization - heatmap\n",
    "    print(\"Creating heatmap...\")\n",
    "    try:\n",
    "        sc.pl.rank_genes_groups_heatmap(adata_hvg, key=\"rank_genes_clusters\", n_genes=10,\n",
    "                              show_gene_labels=True, show=False, swap_axes=True, figsize=(8, 20),\n",
    "                              cmap='viridis', vmin=-3, vmax=3)\n",
    "        plt.savefig('figures/marker_genes_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"Heatmap saved to figures/marker_genes_heatmap.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"Heatmap failed: {e}\")\n",
    "\n",
    "    print(\"Marker gene visualization completed\")\n",
    "\n",
    "# 4.3 Comparison with Supplementary Table 3\n",
    "print(\"\\n4.3 Comparison with Supplementary Table 3\")\n",
    "\n",
    "known_roc_markers = [\n",
    "    \"wnt11b\", \"fzd10\", \"notum\", \"sfrp1\", \"sfrp5\", \"dkk1\", \"axin2\",\n",
    "    \"lef1\", \"tcf7l1\", \"myc\", \"ccnd1\", \"cdkn1a\", \"cdkn1b\", \"tp53\"\n",
    "]\n",
    "\n",
    "print(\"Known ROC marker genes:\")\n",
    "print(known_roc_markers)\n",
    "\n",
    "if 'markers_logistic' in locals():\n",
    "    top_markers = markers_logistic.head(50)['gene'].tolist()\n",
    "    overlap = set(top_markers) & set(known_roc_markers)\n",
    "    print(f\"Overlap with known ROC marker genes: {len(overlap)}\")\n",
    "    print(f\"Overlapping genes: {list(overlap)}\")\n",
    "\n",
    "    overlap_df = pd.DataFrame({\n",
    "        'method': ['logistic_regression'] * len(overlap),\n",
    "        'overlapping_gene': list(overlap)\n",
    "    })\n",
    "    overlap_df.to_csv(\"results/marker_overlap_summary.csv\", index=False)\n",
    "\n",
    "# 4.3.1 Cross-method marker gene validation\n",
    "print(\"\\n4.3.1 Cross-method marker gene validation\")\n",
    "\n",
    "if 'markers_logistic' in locals() and 'markers_wilcoxon' in locals():\n",
    "    # Get top genes from each method\n",
    "    top_logistic = set(markers_logistic.head(50)['gene'].tolist())\n",
    "    top_wilcoxon = set(markers_wilcoxon.head(50)['gene'].tolist())\n",
    "        \n",
    "    # Find overlap\n",
    "    common_markers = top_logistic & top_wilcoxon\n",
    "        \n",
    "    print(f\"\\nMarker gene comparison:\")\n",
    "    print(f\"  Logistic regression (top 50): {len(top_logistic)} genes\")\n",
    "    print(f\"  Wilcoxon test (top 50): {len(top_wilcoxon)} genes\")\n",
    "    print(f\"  Common markers: {len(common_markers)} genes\")\n",
    "    print(f\"  Overlap percentage: {len(common_markers)/50*100:.1f}%\")\n",
    "        \n",
    "    if common_markers:\n",
    "        print(f\"\\nHigh-confidence markers (validated by both methods):\")\n",
    "        # Get details for common markers\n",
    "        common_markers_list = sorted(list(common_markers))\n",
    "        for i, gene in enumerate(common_markers_list[:10], 1):  # Show top 10\n",
    "            logistic_rank = markers_logistic[markers_logistic['gene'] == gene].index[0] + 1\n",
    "            wilcoxon_rank = markers_wilcoxon[markers_wilcoxon['gene'] == gene].index[0] + 1\n",
    "            logistic_score = markers_logistic[markers_logistic['gene'] == gene]['importance'].values[0]\n",
    "            print(f\"  {i}. {gene}\")\n",
    "            print(f\"     - Logistic: rank #{logistic_rank}, score={logistic_score:.3f}\")\n",
    "            print(f\"     - Wilcoxon: rank #{wilcoxon_rank}\")\n",
    "            \n",
    "        # Save common markers\n",
    "        common_markers_df = pd.DataFrame({\n",
    "            'gene': list(common_markers),\n",
    "            'method': 'both'\n",
    "        })\n",
    "        common_markers_df.to_csv(\"results/common_markers_validated.csv\", index=False)\n",
    "        print(f\"\\nCommon markers saved to: results/common_markers_validated.csv\")\n",
    "            \n",
    "        # Create bar chart comparing methods\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "        comparison_data = pd.DataFrame({\n",
    "            'Method': ['Logistic\\nOnly', 'Both\\nMethods', 'Wilcoxon\\nOnly'],\n",
    "            'Count': [\n",
    "                len(top_logistic - common_markers),\n",
    "                len(common_markers),\n",
    "                len(top_wilcoxon - common_markers)\n",
    "            ],\n",
    "            'Color': ['#377eb8', '#4daf4a', '#e41a1c']\n",
    "        })\n",
    "            \n",
    "        bars = ax.bar(comparison_data['Method'], comparison_data['Count'], \n",
    "                     color=comparison_data['Color'], edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "            \n",
    "        ax.set_ylabel('Number of Marker Genes', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('Marker Gene Identification - Method Comparison', fontsize=14, fontweight='bold', pad=20)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{int(height)}',\n",
    "                   ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('figures/marker_method_comparison.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        plt.close()\n",
    "            \n",
    "        print(\"Method comparison plot saved to: figures/marker_method_comparison.png\")\n",
    "    else:\n",
    "        print(\"No common markers found between methods\")\n",
    "else:\n",
    "    print(\"Marker genes not yet identified, skipping validation\")\n",
    "\n",
    "# 4.4 ROC marker genes heatmap visualization\n",
    "print(\"\\n4.4 ROC marker genes heatmap visualization\")\n",
    "\n",
    "if 'markers_logistic' in locals() and cluster_key in adata_hvg.obs:\n",
    "    # Select top 20 ROC marker genes\n",
    "    top_roc_markers = markers_logistic.head(20)['gene'].tolist()\n",
    "\n",
    "    # Ensure genes exist in data\n",
    "    available_markers = [gene for gene in top_roc_markers if gene in adata_hvg.var_names]\n",
    "    print(f\"Using {len(available_markers)} available marker genes for heatmap\")\n",
    "\n",
    "    if available_markers:\n",
    "        # Calculate average expression of marker genes in each cluster\n",
    "        cluster_means = []\n",
    "        cluster_labels = []\n",
    "\n",
    "        for cluster in sorted(adata_hvg.obs[cluster_key].unique()):\n",
    "            cluster_mask = adata_hvg.obs[cluster_key] == cluster\n",
    "            cluster_data = adata_hvg[cluster_mask]\n",
    "\n",
    "            # Calculate average expression of marker genes in this cluster\n",
    "            gene_means = []\n",
    "            for gene in available_markers:\n",
    "                gene_expr = cluster_data[:, gene].X\n",
    "                if hasattr(gene_expr, 'toarray'):\n",
    "                    gene_expr = gene_expr.toarray()\n",
    "                mean_expr = np.mean(gene_expr) if gene_expr.size > 0 else 0\n",
    "                gene_means.append(mean_expr)\n",
    "\n",
    "            cluster_means.append(gene_means)\n",
    "            cluster_labels.append(f\"Cluster {cluster}\")\n",
    "\n",
    "        # Convert to numpy array and transpose to make it vertical\n",
    "        heatmap_data = np.array(cluster_means).T\n",
    "\n",
    "        # Create heatmap\n",
    "        plt.figure(figsize=(10, 16))\n",
    "\n",
    "        # Create heatmap using viridis colormap\n",
    "        sns.heatmap(heatmap_data,\n",
    "                   xticklabels=cluster_labels,\n",
    "                   yticklabels=available_markers,\n",
    "                   cmap='viridis',\n",
    "                   annot=False,\n",
    "                   cbar_kws={'label': 'Mean Expression'})\n",
    "\n",
    "        plt.title('ROC Marker Genes Expression Across Clusters', fontsize=14, pad=20)\n",
    "        plt.xlabel('Clusters', fontsize=12)\n",
    "        plt.ylabel('ROC Marker Genes', fontsize=12)\n",
    "\n",
    "        # Rotate labels for better readability\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('figures/roc_marker_heatmap.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        plt.close()\n",
    "\n",
    "        print(\"ROC marker genes heatmap saved to figures/roc_marker_heatmap.png\")\n",
    "    else:\n",
    "        print(\"Not available\")\n",
    "else:\n",
    "    print(\"Need to identify ROC marker genes and clustering results first\")\n",
    "\n",
    "# 4.5 GO enrichment analysis\n",
    "print(\"\\n4.5 GO enrichment analysis\")\n",
    "\n",
    "try:\n",
    "    import gseapy as gp\n",
    "\n",
    "    if 'markers_logistic' in locals():\n",
    "        roc_genes = markers_logistic.head(50)['gene'].tolist()\n",
    "\n",
    "        print(f\"Number of genes analyzed: {len(roc_genes)}\")\n",
    "        print(f\"Gene list example: {roc_genes[:5]}\")  # Show first 5 genes\n",
    "\n",
    "        # Try with more lenient settings, but expect failure\n",
    "        try:\n",
    "            go_results = gp.enrichr(\n",
    "                gene_list=roc_genes,\n",
    "                gene_sets=['KEGG_2021_Human'],  # Use human KEGG pathways\n",
    "                organism='human',  # Use human genome as reference\n",
    "                outdir='results/go_analysis',\n",
    "                cutoff=0.5  # Further lower threshold\n",
    "            )\n",
    "\n",
    "            # Display results\n",
    "            print(\"\\nKEGG pathway analysis results:\")\n",
    "            if not go_results.results.empty:\n",
    "                print(go_results.results.head(10)[['Term', 'Adjusted P-value', 'Genes']])\n",
    "                # Save results\n",
    "                go_results.results.to_csv(\"results/go_enrichment_results.csv\", index=False)\n",
    "            else:\n",
    "                print(\"No significantly enriched pathways found\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"KEGG analysis also failed: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Need to identify marker genes first for GO analysis\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"GO analysis error: {e}\")\n",
    "\n",
    "# 4.6 Save analysis results\n",
    "print(\"\\n4.6 Save analysis results\")\n",
    "\n",
    "# Create analysis summary\n",
    "analysis_summary = {\n",
    "    \"analysis_type\": [\"roc_cluster_identification\", \"marker_gene_logistic\", \"marker_gene_wilcoxon\", \"go_enrichment\", \"heatmap_visualization\"],\n",
    "    \"completed\": [bool(potential_roc_clusters), 'markers_logistic' in locals(), 'markers_wilcoxon' in locals(), False, 'figures/roc_marker_heatmap.png' in [f.name for f in Path('figures').glob('*')]],\n",
    "    \"output_files\": [\n",
    "        \"results/roc_markers_logistic.csv, results/roc_markers_wilcoxon.csv\",\n",
    "        \"results/roc_markers_logistic.csv\",\n",
    "        \"results/roc_markers_wilcoxon.csv\",\n",
    "        \"results/go_enrichment_results.csv\",\n",
    "        \"figures/roc_marker_heatmap.png\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(analysis_summary)\n",
    "summary_df.to_csv(\"results/analysis_summary.csv\", index=False)\n",
    "\n",
    "print(\"All completed!\")\n",
    "print(\"Generated files:\")\n",
    "print(\"- results/roc_markers_logistic.csv\")\n",
    "print(\"- results/roc_markers_wilcoxon.csv\")\n",
    "print(\"- results/marker_overlap_summary.csv\")\n",
    "print(\"- results/go_enrichment_results.csv (if gseapy available)\")\n",
    "print(\"- figures/roc_marker_heatmap.png\")\n",
    "print(\"- results/analysis_summary.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
